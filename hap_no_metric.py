import cv2
import os
import numpy as np
import time
from dataclasses import dataclass
from typing import Tuple, List
from collections import defaultdict
# from detect_shaft import DETECT_SHAFT
from detect_target import DETECT_TARGET
# from detect_target_2 import DETECT_TARGET_2
from visualize import TargetVisualizer
from collections import defaultdict
from ultralytics import YOLO
import argparse
import re
from scipy.spatial import distance
from scipy.spatial.distance import cdist


class ArcheryPoseEstimator:
    def __init__(self, model_path, source, source1, perspective_file, output_dir):
        """
        ëª¨ë¸ ë° ë°ì´í„° ê²½ë¡œ ì„¤ì •
        """
        self.model = YOLO(model_path)
        self.source = source
        self.source1 = source1
        self.perspective_file = perspective_file
        self.output_dir = output_dir
        self.cnt = 0  # í‚¤í¬ì¸íŠ¸ íŠ¸ë˜í‚¹ì„ ìœ„í•œ ì¹´ìš´í„°
        self.tracked_keypoints = None  # ì´ì „ í”„ë ˆì„ í‚¤í¬ì¸íŠ¸ ì €ì¥ (NumPy ë°°ì—´)
        self.prev_detection_count = None  # ì´ì „ í”„ë ˆì„ ê°ì§€ëœ ê°ì²´ ê°œìˆ˜
        # ê²°ê³¼ ì €ì¥ í´ë” ìƒì„±
        os.makedirs(self.output_dir, exist_ok=True)

        # Perspective ì¢Œí‘œ ë¡œë“œ
        self.perspective_data = self.load_perspective_data()

    def load_perspective_data(self):
        """
        ì›ê·¼ ë³€í™˜ ì¢Œí‘œ ë¡œë“œ
        """
        perspective_data = {}
        with open(self.perspective_file, 'r') as f:
            for line in f:
                parts = line.strip().split(",")
                name, coords = parts[0], list(map(float, parts[1:]))
                perspective_data[name] = np.array(coords, dtype=np.float32).reshape(4, 2)
        return perspective_data

    def apply_perspective_transform(self, keypoints, M):
        """
        ì›ê·¼ ë³€í™˜ í–‰ë ¬ Mì„ ì‚¬ìš©í•˜ì—¬ í‚¤í¬ì¸íŠ¸ ë³€í™˜
        """
        if keypoints is None or len(keypoints) == 0:
            return None

        keypoints = keypoints.reshape(-1, 2)  # (N, 2) í˜•íƒœë¡œ ë³€í™˜
        keypoints_homo = np.hstack([keypoints, np.ones((keypoints.shape[0], 1))])  # (x, y) â†’ (x, y, 1)

        transformed_keypoints = np.dot(M, keypoints_homo.T).T  # ì›ê·¼ ë³€í™˜ ì ìš©
        transformed_keypoints /= transformed_keypoints[:, 2].reshape(-1, 1)  # ì •ê·œí™” (z=1)
        return transformed_keypoints[:, :2]  # (N, 2) ë°˜í™˜

    def indexing(self, transformed_keypoints, threshold=30):
        """
        ê±°ë¦¬ ê¸°ë°˜ íŠ¸ë˜í‚¹ + ì¸ë±ì‹±
        - ì´ì „ ì¢Œí‘œì™€ ë¹„êµí•˜ì—¬ ê°€ì¥ ê°€ê¹Œìš´ ì¢Œí‘œë¥¼ ê°™ì€ ê²ƒìœ¼ë¡œ ê°„ì£¼
        - ìƒˆë¡œìš´ ì¢Œí‘œëŠ” ë¦¬ìŠ¤íŠ¸ì— ì¶”ê°€í•˜ì—¬ ìµœì‹  ìƒíƒœ ìœ ì§€
        - ë§ˆì§€ë§‰ í”„ë ˆì„ì˜ ìµœì‹  ì¢Œí‘œë¥¼ self.tracked_keypointsì—ì„œ í™•ì¸ ê°€ëŠ¥
        - threshold: ê°™ì€ ì¢Œí‘œë¡œ íŒë‹¨í•  ê±°ë¦¬ ê¸°ì¤€ (í”½ì…€ ë‹¨ìœ„)
        """
        transformed_keypoints = np.array(transformed_keypoints).reshape(-1, 2)  # (N, 2) í˜•íƒœë¡œ ë³€í™˜

        if self.tracked_keypoints is None:
            self.tracked_keypoints = transformed_keypoints  # ì²« ì¢Œí‘œ ì €ì¥
            return self.tracked_keypoints[-1]  # ìµœì‹  ì¢Œí‘œ ë°˜í™˜

        prev_keypoints = np.array(self.tracked_keypoints)  # (M, 2)

        distances = cdist(transformed_keypoints, prev_keypoints)  # ê±°ë¦¬ í–‰ë ¬ ê³„ì‚°
        matched_indices = {}

        for i, row in enumerate(distances):
            min_dist = np.min(row)
            min_idx = np.argmin(row)
            if min_dist < threshold:
                matched_indices[i] = min_idx

        updated_keypoints = np.zeros_like(prev_keypoints)
        updated_mask = np.zeros(prev_keypoints.shape[0], dtype=bool)

        for curr_idx, prev_idx in matched_indices.items():
            updated_keypoints[prev_idx] = transformed_keypoints[curr_idx]
            updated_mask[prev_idx] = True

        new_keypoints = transformed_keypoints[
            ~np.isin(range(len(transformed_keypoints)), list(matched_indices.keys()))
        ]
        self.tracked_keypoints = np.vstack([updated_keypoints[updated_mask], new_keypoints])

        print(f"ë§ˆì§€ë§‰ ì¢Œí‘œ : {self.tracked_keypoints[-1]}")
        return self.tracked_keypoints[-1]  # ìµœì‹  ì¢Œí‘œ ë°˜í™˜

    def process_images(self):
        """
        YOLO ëª¨ë¸ì„ ì´ìš©í•´ ì´ë¯¸ì§€ ì˜ˆì¸¡ í›„, ì›ê·¼ ë³€í™˜ ì ìš© ë° ì €ì¥
        """
        # ì´ë¯¸ì§€ íŒŒì¼ ëª©ë¡ ì •ë ¬
        image_files_cam1 = [os.path.join(self.source, f) for f in sorted(os.listdir(self.source)) if f.endswith(('.png', '.jpg'))]
        image_files_cam3 = [os.path.join(self.source1, f) for f in sorted(os.listdir(self.source1)) if f.endswith(('.png', '.jpg'))]

        # YOLO ì˜ˆì¸¡ ì‹¤í–‰
        results_cam1 = self.model.predict(image_files_cam1, batch=16, device="cuda", save=True, line_width=1, project=self.output_dir)
        results_cam3 = self.model.predict(image_files_cam3, batch=16, device="cuda", save=True, line_width=1, project=self.output_dir)

        for img_path_cam1, result_cam1, img_path_cam3, result_cam3 in zip(image_files_cam1, results_cam1, image_files_cam3, results_cam3):
            img_name_cam1 = os.path.basename(img_path_cam1)
            img_name_cam3 = os.path.basename(img_path_cam3)

            img_cam1 = cv2.imread(img_path_cam1)
            img_cam3 = cv2.imread(img_path_cam3)

            keypoints_cam1 = result_cam1.keypoints.xy.cpu().numpy() if result_cam1.keypoints is not None else None
            keypoints_cam3 = result_cam3.keypoints.xy.cpu().numpy() if result_cam3.keypoints is not None else None

            # ê°ì§€ëœ ê°œìˆ˜ ê³„ì‚°
            detection_count_cam1 = len(keypoints_cam1) if keypoints_cam1 is not None else 0
            detection_count_cam3 = len(keypoints_cam3) if keypoints_cam3 is not None else 0

            # CAM1ì—ì„œ ìƒˆë¡œìš´ ê°ì²´ê°€ ê°ì§€ë˜ì§€ ì•Šì•˜ì§€ë§Œ CAM3ì—ì„œ ê°ì§€ë¨ â†’ CAM3 ì‚¬ìš©
            if self.prev_detection_count == detection_count_cam1 and detection_count_cam3 > detection_count_cam1:
                print(f"{img_name_cam1}: CAM1ì—ì„œ ìƒˆë¡œìš´ í™”ì‚´ ê°ì§€ ì•ˆë¨ â†’ CAM3 ê²°ê³¼ ì‚¬ìš©")
                selected_keypoints = keypoints_cam3
                selected_img = img_cam3
                selected_img_name = img_name_cam3

            # CAM1ì—ì„œ ê°ì§€ëœ ê²½ìš° â†’ CAM1 ê²°ê³¼ ì‚¬ìš©
            elif keypoints_cam1 is not None and keypoints_cam1.size > 0:
                print(f"{img_name_cam1}: CAM1ì—ì„œ ê°ì²´ ê²€ì¶œë¨")
                selected_keypoints = keypoints_cam1
                selected_img = img_cam1
                selected_img_name = img_name_cam1

            # CAM1ê³¼ CAM3 ëª¨ë‘ ê²€ì¶œë˜ì§€ ì•Šì€ ê²½ìš° â†’ ìŠ¤í‚µ
            else:
                print(f"{img_name_cam1}: CAM1, CAM3 ëª¨ë‘ì—ì„œ ê°ì²´ ê²€ì¶œ ì•ˆë¨ â†’ ê±´ë„ˆëœ€")
                continue
            # íŒŒì¼ëª…ì—ì„œ `_frame_ìˆ«ì.í™•ì¥ì` ì œê±°í•˜ì—¬ ê¸°ì¤€ ì´ë¦„ ì¶”ì¶œ
            base_name = re.sub(r"_frame_\d+\.(png|jpg)$", "", selected_img_name)
            # perspective_dataì— í•´ë‹¹ ì´ë¦„ì´ ìˆëŠ” ê²½ìš° ì›ê·¼ ë³€í™˜ ì ìš©
            if base_name in self.perspective_data.keys():
                perspective_coords = self.perspective_data[base_name]

                # 1920Ã—1920 í•´ìƒë„ë¡œ ì›ê·¼ ë³€í™˜
                dst_points = np.float32([
                    [0, 0], [1919, 0], [1919, 1919], [0, 1919]
                ])
                M = cv2.getPerspectiveTransform(perspective_coords, dst_points)

                # ì´ë¯¸ì§€ ì›ê·¼ ë³€í™˜ ì ìš© (1920Ã—1920)
                transformed_img = cv2.warpPerspective(selected_img, M, (1920, 1920))

                # í‚¤í¬ì¸íŠ¸ì— ì›ê·¼ ë³€í™˜ ì ìš©
                transformed_keypoints = self.apply_perspective_transform(selected_keypoints, M)
                latest_keypoint = self.indexing(transformed_keypoints)
            

                # ë³€í™˜ëœ í‚¤í¬ì¸íŠ¸ì—ì„œ x, y ì¢Œí‘œ ì¶”ì¶œ
                x, y = latest_keypoint
                #=======================================================================================================
                # # TODO: x,y ì¢Œí‘œê°€ ë§ˆì§€ë§‰ ì¢Œí‘œë¡œ ì˜ ëëŠ” ì§€ ë””ë²„ê¹…
                # # ğŸ”¹ í‚¤í¬ì¸íŠ¸ ì‹œê°í™” (ë””ë²„ê¹…ìš©)
                # for i, (px, py) in enumerate(transformed_keypoints):
                #     cv2.circle(transformed_img, (int(px), int(py)), 5, (0, 255, 0), -1)  # ì´ˆë¡ìƒ‰ (ëª¨ë“  ë³€í™˜ëœ í‚¤í¬ì¸íŠ¸)
                #     cv2.putText(transformed_img, f"{i}", (int(px) + 5, int(py) - 5), 
                #                 cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 1, cv2.LINE_AA)  # ë²ˆí˜¸ í‘œì‹œ

                # # ğŸ”¹ ìµœì¢… ì„ íƒëœ í‚¤í¬ì¸íŠ¸ ì‹œê°í™” (ë¹¨ê°„ìƒ‰)
                # cv2.circle(transformed_img, (int(x), int(y)), 8, (0, 0, 255), -1)  # ë¹¨ê°„ìƒ‰ (ìµœì¢… ì„ íƒëœ í‚¤í¬ì¸íŠ¸)
                # cv2.putText(transformed_img, "Final", (int(x) + 10, int(y) - 10), 
                #             cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 255), 1, cv2.LINE_AA)  # "Final" í…ìŠ¤íŠ¸ ì¶”ê°€

                # # ê¸°ì¡´ tracked_keypoints ì‹œê°í™” (íŒŒë€ìƒ‰)
                # if self.tracked_keypoints is not None:
                #     for i, (tx, ty) in enumerate(self.tracked_keypoints):
                #         cv2.circle(transformed_img, (int(tx), int(ty)), 4, (255, 0, 0), -1)  # íŒŒë€ìƒ‰ (ê¸°ì¡´ íŠ¸ë˜í‚¹ëœ í‚¤í¬ì¸íŠ¸)
                #         cv2.putText(transformed_img, f"T{i}", (int(tx) + 5, int(ty) - 5),
                #                     cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 1, cv2.LINE_AA)  # "T" + ë²ˆí˜¸

                # # ì´ë¯¸ì§€ ì €ì¥ (ë””ë²„ê¹…ìš©)
                debug_img_path = os.path.join(self.output_dir, f"debug_{selected_img_name}")
                cv2.imwrite(debug_img_path, transformed_img)
                print(f"âœ… Debug Image Saved: {debug_img_path}")

                # # ë³€í™˜ëœ ì´ë¯¸ì§€ ì €ì¥
                bg_image_path = os.path.join(self.output_dir, f"perspective_{selected_img_name}")
                cv2.imwrite(bg_image_path, transformed_img)
                print(f"{bg_image_path} ì €ì¥ ì™„ë£Œ!")
                # =======================================================================================================
                
                
                # print(transformed_keypoints)
                hits = []
                # score_target = []
                for shaft_coord in transformed_keypoints:
                    x, y = shaft_coord  

                    # ê° í™”ì‚´ì˜ ìœ„ì¹˜ë³„ë¡œ DETECT_TARGETì„ ì‹¤í–‰í•˜ì—¬ ì ìˆ˜ë¥¼ ê³„ì‚°
                    target_detector = DETECT_TARGET(
                        bg_image_path,
                        int(x),
                        int(y),
                        min_area=5000,
                        max_area=1000000,
                        center_tolerance=300,
                        max_ellipses=15,
                    )       
                    center, score, merged_ellipses = target_detector.process_target_detection()

                    hits.append(
                        {
                            "point": (int(x), int(y)),
                            "score": score,  # ê° ì¢Œí‘œë³„ ê°œë³„ì ì¸ score ì ìš©
                        }
                    )
                    


                # # Create the visualizer
                visualizer = TargetVisualizer(center[0], center[1])
                print(center)

                # # Example hit points with scores
                # hits = [{"point": (x, y), "score": score}]

                # Draw the visualization
                output_img = visualizer.visualize(transformed_img, hits)
                output_path = os.path.join(self.output_dir, f"score4_{selected_img_name}")
                cv2.imwrite(output_path, transformed_img)

                # Display the image
                cv2.imshow("Visualization", output_img)
                cv2.waitKey(0)
                cv2.destroyAllWindows()
                
            self.prev_detection_count = detection_count_cam1


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default='./pose_s_add_best.pt')
    parser.add_argument("--source", type=str, default='../labeling/archery_20250116/20250116_091103/cam1_4set/')
    parser.add_argument("--source1", type=str, default='../labeling/archery_20250116/20250116_091103/cam2_4set/')
    parser.add_argument("--perspective", type=str, default='../labeling/archery_20250116/20250116.txt')
    parser.add_argument("--output", type=str, default='./output_results')

    args = parser.parse_args()

    estimator = ArcheryPoseEstimator(
        model_path=args.model,
        source=args.source,
        source1=args.source1,
        perspective_file=args.perspective,
        output_dir=args.output
    )

    estimator.process_images()

    
