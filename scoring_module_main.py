import cv2
import os
import numpy as np
import re
import json
import argparse
from collections import deque
from scipy.spatial.distance import cdist
from ultralytics import YOLO
from scoring_module.detect_target import DETECT_TARGET
from scoring_module.assign_score import ASSIGN_SCORE
from scoring_module.visualize import TargetVisualizer


# 1. ì„¸íŠ¸ë³„ íŒŒì¼ ì½ê¸° í´ë˜ìŠ¤
class SetFileReader:
    def __init__(self, source_cam1, source_cam3):
        self.source_cam1 = source_cam1
        self.source_cam3 = source_cam3

    def get_image_files(self):
        files_cam1 = sorted(
            [
                os.path.join(self.source_cam1, f)
                for f in os.listdir(self.source_cam1)
                if f.lower().endswith((".png", ".jpg"))
            ]
        )
        files_cam3 = sorted(
            [
                os.path.join(self.source_cam3, f)
                for f in os.listdir(self.source_cam3)
                if f.lower().endswith((".png", ".jpg"))
            ]
        )
        return files_cam1, files_cam3

    def get_current_set_name(self, filename):
        # íŒŒì¼ ì´ë¦„ì—ì„œ ì„¸íŠ¸ ì´ë¦„ ì¶”ì¶œ (ì˜ˆ: "20250116_091103_cam1_1set")
        parts = filename.split("_")
        return "_".join(parts[:5])


# 2. í™”ì‚´ ì¢Œí‘œ ì¶”ì¶œ ë° ë³€í™˜ ì´ë¯¸ì§€ ì €ì¥ í´ë˜ìŠ¤
class PerspectiveTransformer:
    def __init__(self, perspective_file, output_dir):
        self.perspective_file = perspective_file
        self.output_dir = output_dir
        self.perspective_data = self.load_perspective_data()
        self.tracked_keypoints = None  # í™”ì‚´ ì¢Œí‘œ íŠ¸ë˜í‚¹

    def load_perspective_data(self):
        perspective_data = {}
        with open(self.perspective_file, "r") as f:
            for line in f:
                parts = line.strip().split(",")
                name = parts[0]
                coords = list(map(float, parts[1:]))
                perspective_data[name] = np.array(coords, dtype=np.float32).reshape(
                    4, 2
                )
        return perspective_data

    def apply_perspective_transform(self, image, keypoints, base_name):
        if base_name in self.perspective_data:
            perspective_coords = self.perspective_data[base_name]
            dst_points = np.float32([[0, 0], [1919, 0], [1919, 1919], [0, 1919]])
            M = cv2.getPerspectiveTransform(perspective_coords, dst_points)
            transformed_img = cv2.warpPerspective(image, M, (1920, 1920))
            transformed_keypoints = None
            if keypoints is not None and len(keypoints) > 0:
                transformed_keypoints = self._transform_keypoints(keypoints, M)
            return transformed_img, transformed_keypoints, M
        else:
            # perspective_dataì— í•´ë‹¹ ì´ë¦„ì´ ì—†ìœ¼ë©´ ì›ë³¸ ë°˜í™˜
            return image, keypoints, None

    def _transform_keypoints(self, keypoints, M):
        keypoints = keypoints.reshape(-1, 2)
        keypoints_homo = np.hstack([keypoints, np.ones((keypoints.shape[0], 1))])
        transformed = np.dot(M, keypoints_homo.T).T
        transformed = transformed / transformed[:, 2].reshape(-1, 1)
        return transformed[:, :2]

    def index_keypoints(self, transformed_keypoints, threshold=30):
        transformed_keypoints = np.array(transformed_keypoints).reshape(-1, 2)
        if self.tracked_keypoints is None:
            self.tracked_keypoints = transformed_keypoints
            return self.tracked_keypoints[-1]
        prev_keypoints = np.array(self.tracked_keypoints)
        distances = cdist(transformed_keypoints, prev_keypoints)
        matched_indices = {}
        for i, row in enumerate(distances):
            min_dist = np.min(row)
            min_idx = np.argmin(row)
            if min_dist < threshold:
                matched_indices[i] = min_idx
        updated_keypoints = np.zeros_like(prev_keypoints)
        updated_mask = np.zeros(prev_keypoints.shape[0], dtype=bool)
        for curr_idx, prev_idx in matched_indices.items():
            updated_keypoints[prev_idx] = transformed_keypoints[curr_idx]
            updated_mask[prev_idx] = True
        new_keypoints = transformed_keypoints[
            ~np.isin(range(len(transformed_keypoints)), list(matched_indices.keys()))
        ]
        self.tracked_keypoints = np.vstack(
            [updated_keypoints[updated_mask], new_keypoints]
        )
        print(f"Latest tracked keypoint: {self.tracked_keypoints[-1]}")
        return self.tracked_keypoints[-1]

    def save_transformed_image(self, transformed_img, filename):
        os.makedirs(self.output_dir, exist_ok=True)
        path = os.path.join(self.output_dir, f"perspective_{filename}")
        cv2.imwrite(path, transformed_img)
        # print(f"Transformed image saved: {path}")
        return path


# 3. TargetDetectorë¥¼ í™œìš©í•œ ì ìˆ˜ì˜ì—­ ì»¨íˆ¬ì–´ ê²€ì¶œ í´ë˜ìŠ¤
class TargetDetector:
    def __init__(
        self,
        circularity_threshold=0.85,
        min_area=10000,
        max_area=10000000,
        min_length=500.0,
        max_length=20000.0,
        center_tolerance=300,
        max_contours_count=10,
    ):
        self.circularity_threshold = circularity_threshold
        self.min_area = min_area
        self.max_area = max_area
        self.min_length = min_length
        self.max_length = max_length
        self.center_tolerance = center_tolerance
        self.max_contours_count = max_contours_count

    def get_contours(self, image_path):
        target_detector = DETECT_TARGET(
            image_path,
            circularity_threshold=self.circularity_threshold,  # ì›í˜•ë„ ì„ê³„ê°’
            min_area=self.min_area,  # ë„ˆë¬´ ì‘ì€ ì˜ì—­ì€ ì œê±°
            max_area=self.max_area,  # ë„ˆë¬´ í° ì˜ì—­ì€ ì œê±°
            min_length=self.min_length,  # ìœ¤ê³½ ê¸¸ì´(ë‘˜ë ˆ)ê°€ ë„ˆë¬´ ì§§ì€ ê²ƒ ì œê±°
            max_length=self.max_length,  # ë„ˆë¬´ ê¸´ ê²ƒë„ ì œê±°
            center_tolerance=self.center_tolerance,  # ì¤‘ì‹¬ ì¢Œí‘œ í—ˆìš© ì˜¤ì°¨
            max_contours_count=self.max_contours_count,  # ìµœëŒ€ ì»¨íˆ¬ì–´ ê°œìˆ˜
            # debug = False,  # ë””ë²„ê¹… ëª¨ë“œ
        )
        (cX_0, cY_0), contours_of_points = target_detector.process_target_detection()

        return (cX_0, cY_0), contours_of_points


# 4. Scorerë¥¼ í™œìš©í•œ ì ìˆ˜ í• ë‹¹ í´ë˜ìŠ¤
class Scorer:
    def __init__(self):
        self.scoring = ASSIGN_SCORE()

    def get_score(self, center, arrow, contours_of_points):
        """
        ì¤‘ì‹¬ ì¢Œí‘œì™€ í™”ì‚´ ì¢Œí‘œ, ê·¸ë¦¬ê³  ì ìˆ˜ë³„ ì»¨íˆ¬ì–´(í´ë¦¬ê³¤)ë¥¼ ì´ìš©í•˜ì—¬
        í™”ì‚´ì˜ ìœ„ì¹˜ì— ë”°ë¥¸ ì„¸ë¶„í™”ëœ ì ìˆ˜ë¥¼ ì„ í˜• ë³´ê°„ ë°©ì‹ìœ¼ë¡œ í• ë‹¹í•˜ëŠ” í•¨ìˆ˜.

        Parameters:
            center (tuple): (cX_0, cY_0) ì¤‘ì‹¬ ì¢Œí‘œ
            arrow (tuple): í™”ì‚´ ì¢Œí‘œ (shaft position)
            contours_of_points (list of ndarray): ì ìˆ˜ë³„ ì»¨íˆ¬ì–´(í´ë¦¬ê³¤) ëª©ë¡
                                                (ì˜ˆ: cv2.findContours ê²°ê³¼)

        Returns:
            score (float or None): ìµœì¢… í• ë‹¹ ì ìˆ˜ (êµì°¨ì  ê³„ì‚°ì´ ì•ˆë˜ë©´ fallbackìœ¼ë¡œ ë©´ì  ê¸°ë°˜ ê³„ì‚°)
        """
        score = self.scoring.assign_score(center, arrow, contours_of_points)

        return score


# 5. ì •í™•ë„ ê²€ì¦ ë° ê²°ê³¼ JSON ì €ì¥ í´ë˜ìŠ¤
class AccuracyValidator:
    def __init__(
        self,
        json_truth_file="./updated_truth_annotations.json",
        json_output_file="./upgrade_score_results.json",
    ):
        self.json_truth_file = json_truth_file
        self.json_output_file = json_output_file
        self.tp, self.fp, self.tn, self.fn = 0, 0, 0, 0
        self.set_sum = 0
        self.prev_detected_count = 0
        self.existing_scores = self.load_existing_scores()

    def load_existing_scores(self):
        if os.path.exists(self.json_truth_file):
            with open(self.json_truth_file, "r") as f:
                return json.load(f)
        return {}

    def get_existing_score(self, img_name):
        return self.existing_scores.get(img_name, {}).get("score", 0)

    def update_metrics(self, img_name, detected_count, accumulated_score):
        truth_score = self.get_existing_score(img_name)
        detected_increase = detected_count - self.prev_detected_count
        # 1. ê²€ì¶œ ì„±ê³µ & ì ìˆ˜ ì •í™•
        if truth_score > 0 and detected_count > 0 and accumulated_score == truth_score:
            self.tp += 1
            print(f"âœ… TP: {img_name} - ê²€ì¶œ ì„±ê³µ (ì ìˆ˜ ì •í™•)")
        # 2. ì‹¤ì œ í™”ì‚´ ìˆìŒ â†’ ê²€ì¶œ ì•ˆë¨
        elif truth_score > self.set_sum and detected_count <= self.prev_detected_count:
            self.fn += 1
            print(f"âŒ FN: {img_name} - ì ìˆ˜ ìˆìŒ({truth_score}) â†’ ê²€ì¶œ ì•ˆë¨")
        # 3. ì ìˆ˜ ì—†ìŒì¸ë° ê²€ì¶œë¨ (ë…¸ì´ì¦ˆ ê°€ëŠ¥)
        elif truth_score == 0 and detected_count > 0:
            self.fp += 1
            print(f"ğŸš¨ FP: {img_name} - ì ìˆ˜ ì—†ìŒì¸ë° ê²€ì¶œë¨")
        # 4. ê°‘ì‘ìŠ¤ëŸ° í™”ì‚´ ê°œìˆ˜ ì¦ê°€ í˜¼í•© ê²½ìš°
        elif detected_increase >= 2:
            if accumulated_score == truth_score:
                self.tp += 1
                print(f"âœ… TP: {img_name} - ê²€ì¶œ ì„±ê³µ (ì ìˆ˜ ì •í™•)")
            else:
                self.fp += 1
                print(f"ğŸš¨ FP: {img_name} - ê°‘ì‘ìŠ¤ëŸ° í™”ì‚´ ê°œìˆ˜ ì¦ê°€ (ë…¸ì´ì¦ˆ ê°€ëŠ¥)")
        # 5. ê²€ì¶œëì§€ë§Œ ì ìˆ˜ ë¶ˆì¼ì¹˜
        elif detected_count > 0 and accumulated_score != truth_score:
            if accumulated_score > truth_score:
                self.fp += 1
                print(f"ğŸš¨ FP: {img_name} - ê²€ì¶œëìœ¼ë‚˜ ê³¼ê²€ì¶œ")
            else:
                self.fn += 1
                print(f"âŒ FN: {img_name} - ê²€ì¶œëìœ¼ë‚˜ ë¯¸ê²€ì¶œ")
        # 6. ì ìˆ˜ ì—†ìŒ & ê²€ì¶œ ì•ˆë¨
        elif truth_score == 0 and detected_count == 0:
            self.tn += 1
            print(f"ğŸŸ¢ TN: {img_name} - ì ìˆ˜ ì—†ìŒ, ê²€ì¶œë„ ì•ˆë¨")
        self.prev_detected_count = detected_count
        self.set_sum = truth_score
        return truth_score

    def compute_metrics(self):
        precision = self.tp / (self.tp + self.fp + 1e-6)
        recall = self.tp / (self.tp + self.fn + 1e-6)
        accuracy = (self.tp + self.tn) / (self.tp + self.fp + self.fn + self.tn + 1e-6)
        return precision, recall, accuracy

    def save_results(self, img_name, truth_score, detected_count, accumulated_score):
        precision, recall, accuracy = self.compute_metrics()
        score_results = {
            "truth_score": truth_score,
            "detected_count": detected_count,
            "TP": self.tp,
            "FP": self.fp,
            "TN": self.tn,
            "FN": self.fn,
            "Precision": precision,
            "Recall": recall,
            "Accuracy": accuracy,
        }
        if os.path.exists(self.json_output_file):
            try:
                with open(self.json_output_file, "r") as f:
                    existing_data = json.load(f)
            except json.JSONDecodeError:
                existing_data = {}
        else:
            existing_data = {}
        existing_data[img_name] = score_results
        with open(self.json_output_file, "w") as f:
            json.dump(existing_data, f, indent=4, ensure_ascii=False)
        print(f"âœ… ê²°ê³¼ê°€ {self.json_output_file} íŒŒì¼ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤!")


# main í•¨ìˆ˜: ê° í´ë˜ìŠ¤ë¥¼ í™œìš©í•˜ì—¬ ì „ì²´ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
def main(args):
    # YOLO ëª¨ë¸ ë¡œë“œ
    model = YOLO(args.model)

    # í´ë˜ìŠ¤ ì¸ìŠ¤í„´ìŠ¤ ìƒì„±
    file_reader = SetFileReader(args.source, args.source1)
    transformer = PerspectiveTransformer(args.perspective, args.output)
    target_detector = TargetDetector(
        circularity_threshold=0.85,
        min_area=10000,
        max_area=10000000,
        min_length=500.0,
        max_length=20000.0,
        center_tolerance=300,
        max_contours_count=10,
    )
    scorer = Scorer()
    validator = AccuracyValidator()

    image_files_cam1, image_files_cam3 = file_reader.get_image_files()

    # YOLO ì˜ˆì¸¡ (CAM1, CAM3)
    results_cam1 = model.predict(
        image_files_cam1,
        batch=16,
        device="cuda",
        save=True,
        line_width=1,
        project=args.output,
    )
    results_cam3 = model.predict(
        image_files_cam3,
        batch=16,
        device="cuda",
        save=True,
        line_width=1,
        project=args.output,
    )

    contours_list = deque(maxlen=10)
    hits = []
    for img_path_cam1, result_cam1, img_path_cam3, result_cam3 in zip(
        image_files_cam1, results_cam1, image_files_cam3, results_cam3
    ):
        img_name_cam1 = os.path.basename(img_path_cam1)
        img_name_cam3 = os.path.basename(img_path_cam3)

        current_set = file_reader.get_current_set_name(img_name_cam1)
        print(f"í˜„ì¬ ì„¸íŠ¸: {current_set}")

        # ì´ë¯¸ì§€ ë¡œë“œ
        img_cam1 = cv2.imread(img_path_cam1)
        img_cam3 = cv2.imread(img_path_cam3)

        # CAM1ì—ì„œ í™”ì‚´ ê²€ì¶œ ê²°ê³¼ê°€ ìˆìœ¼ë©´ ìš°ì„  ì‚¬ìš©
        if (
            result_cam1.keypoints is not None
            and result_cam1.keypoints.xy.cpu().numpy().size > 0
        ):
            keypoints = result_cam1.keypoints.xy.cpu().numpy()
            selected_img = img_cam1
            selected_img_name = img_name_cam1
            detected_count = len(keypoints)
            print(f"{selected_img_name}: CAM1ì—ì„œ ê°ì²´ ê²€ì¶œë¨")
        else:
            keypoints = None
            selected_img = img_cam1  # ê¸°ë³¸ì ìœ¼ë¡œ CAM1 ì´ë¯¸ì§€ ì‚¬ìš©
            selected_img_name = img_name_cam1
            detected_count = 0
            print(f"{selected_img_name}: CAM1, CAM3 ëª¨ë‘ì—ì„œ ê°ì²´ ê²€ì¶œ ì•ˆë¨")

        # íŒŒì¼ ì´ë¦„ì—ì„œ "_frame_ìˆ«ì" ì œê±°í•˜ì—¬ ê¸°ì¤€ ì´ë¦„ ì¶”ì¶œ
        base_name = re.sub(
            r"_frame_\d+\.(png|jpg)$", "", os.path.basename(img_path_cam1)
        )
        # ì›ê·¼ ë³€í™˜ ì ìš© ë° í™”ì‚´ ì¢Œí‘œ ë³€í™˜
        transformed_img, transformed_keypoints, _ = (
            transformer.apply_perspective_transform(selected_img, keypoints, base_name)
        )
        if transformed_keypoints is not None:
            latest_keypoint = transformer.index_keypoints(transformed_keypoints)
            x, y = latest_keypoint
        else:
            x, y = 0, 0

        # ë³€í™˜ëœ ì´ë¯¸ì§€ ì €ì¥
        transformed_img_path = transformer.save_transformed_image(
            transformed_img, selected_img_name
        )
        # print(f"ë³€í™˜ëœ ì´ë¯¸ì§€ ê²½ë¡œ: {transformed_img_path}")

        # TargetScorerë¡œ ì ìˆ˜ ì‚°ì¶œ
        center, contours = target_detector.get_contours(transformed_img_path)
        if contours != None:  # ì»¨íˆ¬ì–´ë¥¼ 10ê°œ ì°¾ì•˜ìœ¼ë©´ ì¶”ê°€
            print("$$$ìƒˆë¡œìš´ ì»¨íˆ¬ì–´ ì‚¬ìš©$$$")
            score = scorer.get_score(center, [(x, y)], contours)
            for c in contours:
                contours_list.append(c)
        else:
            print("@@@ë°±ì—… ì»¨íˆ¬ì–´ ì‚¬ìš©@@@")
            score = scorer.get_score(center, [(x, y)], contours_list)
        # print(f"{selected_img_name}: ì ìˆ˜ = {score}")
        hits.append(
            {
                "point": (
                    int(x),
                    int(y),
                ),
                "score": score,
            }
        )
        # Example usage:
        img = cv2.imread(transformed_img_path)

        # Create the visualizer
        C_x, C_y = center
        visualizer = TargetVisualizer(int(C_x), int(C_y))

        # Draw the visualization
        output_img = visualizer.visualize(img, hits)

        # Display the image
        cv2.imshow("Visualization", output_img)
        cv2.waitKey(0)
        cv2.destroyAllWindows()

        # AccuracyValidatorë¡œ ê²€ì¶œ ê²°ê³¼ ì—…ë°ì´íŠ¸ ë° JSON ì €ì¥
        accumulated_score = score  # ì—¬ê¸°ì„œëŠ” ë‹¨ìˆœíˆ í•´ë‹¹ í”„ë ˆì„ì˜ ì ìˆ˜ë¥¼ ì‚¬ìš© (ëˆ„ì  ë°©ì‹ì€ í•„ìš”ì— ë”°ë¼ ìˆ˜ì •)
        truth_score = validator.update_metrics(
            selected_img_name, detected_count, accumulated_score
        )
        validator.save_results(
            selected_img_name, truth_score, detected_count, accumulated_score
        )


if __name__ == "__main__":
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default="./models/pose_s_add_best.pt")
    parser.add_argument(
        "--source",
        type=str,
        default="./testset/20250116_091103/cam1_4set/",
    )
    parser.add_argument(
        "--source1",
        type=str,
        default="./testset/20250116_091103/cam2_4set/",
    )
    parser.add_argument(
        "--perspective", type=str, default="./perspective_coordinates/20250116.txt"
    )
    parser.add_argument("--output", type=str, default="./output_results")
    args = parser.parse_args()
    main(args)
